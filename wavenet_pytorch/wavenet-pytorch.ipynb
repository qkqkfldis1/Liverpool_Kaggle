{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import os\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0\"  # specify which GPU(s) to be used\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations and main hyperparammeters\n",
    "EPOCHS = 150\n",
    "NNBATCHSIZE = 16\n",
    "GROUP_BATCH_SIZE = 4000\n",
    "SEED = 321\n",
    "LR = 0.001\n",
    "SPLITS = 5\n",
    "\n",
    "outdir = 'wavenet_models'\n",
    "flip = False\n",
    "noise = False\n",
    "\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batches of 4000 observations\n",
    "def batching(df, batch_size):\n",
    "    #print(df)\n",
    "    df['group'] = df.groupby(df.index//batch_size, sort=False)['signal'].agg(['ngroup']).values\n",
    "    df['group'] = df['group'].astype(np.uint16)\n",
    "    return df\n",
    "\n",
    "# normalize the data (standard scaler). We can also try other scalers for a better score!\n",
    "def normalize(train, test):\n",
    "    train_input_mean = train.signal.mean()\n",
    "    train_input_sigma = train.signal.std()\n",
    "    train['signal'] = (train.signal - train_input_mean) / train_input_sigma\n",
    "    test['signal'] = (test.signal - train_input_mean) / train_input_sigma\n",
    "    return train, test\n",
    "\n",
    "# get lead and lags features\n",
    "def lag_with_pct_change(df, windows):\n",
    "    for window in windows:    \n",
    "        df['signal_shift_pos_' + str(window)] = df.groupby('group')['signal'].shift(window).fillna(0)\n",
    "        df['signal_shift_neg_' + str(window)] = df.groupby('group')['signal'].shift(-1 * window).fillna(0)\n",
    "    return df\n",
    "\n",
    "# main module to run feature engineering. Here you may want to try and add other features and check if your score imporves :).\n",
    "def run_feat_engineering(df, batch_size):\n",
    "    # create batches\n",
    "    df = batching(df, batch_size = batch_size)\n",
    "    # create leads and lags (1, 2, 3 making them 6 features)\n",
    "    df = lag_with_pct_change(df, [1, 2, 3])\n",
    "    # create signal ** 2 (this is the new feature)\n",
    "    df['signal_2'] = df['signal'] ** 2\n",
    "    return df\n",
    "\n",
    "# fillna with the mean and select features for training\n",
    "def feature_selection(train, test):\n",
    "    features = [col for col in train.columns if col not in ['index', 'group', 'open_channels', 'time']]\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)\n",
    "    for feature in features:\n",
    "        feature_mean = pd.concat([train[feature], test[feature]], axis = 0).mean()\n",
    "        train[feature] = train[feature].fillna(feature_mean)\n",
    "        test[feature] = test[feature].fillna(feature_mean)\n",
    "    return train, test, features\n",
    "\n",
    "\n",
    "def split(GROUP_BATCH_SIZE=4000, SPLITS=5):\n",
    "    print('Reading Data Started...')\n",
    "    train, test, sample_submission = read_data()\n",
    "    train, test = normalize(train, test)\n",
    "    print('Reading and Normalizing Data Completed')\n",
    "    print('Creating Features')\n",
    "    print('Feature Engineering Started...')\n",
    "    train = run_feat_engineering(train, batch_size=GROUP_BATCH_SIZE)\n",
    "    test = run_feat_engineering(test, batch_size=GROUP_BATCH_SIZE)\n",
    "    train, test, features = feature_selection(train, test)\n",
    "    print(train.head())\n",
    "    print('Feature Engineering Completed...')\n",
    "\n",
    "    target = ['open_channels']\n",
    "    group = train['group']\n",
    "    kf = GroupKFold(n_splits=SPLITS)\n",
    "    splits = [x for x in kf.split(train, train[target], group)]\n",
    "    new_splits = []\n",
    "    for sp in splits:\n",
    "        new_split = []\n",
    "        new_split.append(np.unique(group[sp[0]]))\n",
    "        new_split.append(np.unique(group[sp[1]]))\n",
    "        new_split.append(sp[1])\n",
    "        new_splits.append(new_split)\n",
    "    target_cols = ['open_channels']\n",
    "    print(train.head(), train.shape)\n",
    "    train_tr = np.array(list(train.groupby('group').apply(lambda x: x[target_cols].values))).astype(np.float32)\n",
    "    train = np.array(list(train.groupby('group').apply(lambda x: x[features].values)))\n",
    "    test = np.array(list(test.groupby('group').apply(lambda x: x[features].values)))\n",
    "    print(train.shape, test.shape, train_tr.shape)\n",
    "    return train, test, train_tr, new_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavenet \n",
    "class wave_block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=int((kernel_size + (kernel_size-1)*(dilation-1))/2), dilation=dilation)\n",
    "        self.conv3 = nn.Conv1d(out_ch, out_ch, kernel_size, padding=int((kernel_size + (kernel_size-1)*(dilation-1))/2), dilation=dilation)\n",
    "        self.conv4 = nn.Conv1d(out_ch, out_ch, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_x = x\n",
    "        tanh = self.tanh(self.conv2(x))\n",
    "        sig = self.sigmoid(self.conv3(x))\n",
    "        res = tanh.mul(sig)\n",
    "        x = self.conv4(res)\n",
    "        x = res_x + x\n",
    "        return x\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, basic_block=wave_block):\n",
    "        super().__init__()\n",
    "        self.basic_block = basic_block\n",
    "        self.layer1 = self._make_layers(8, 16, 3, 12)\n",
    "        self.layer2 = self._make_layers(16, 32, 3, 8)\n",
    "        self.layer3 = self._make_layers(32, 64, 3, 4)\n",
    "        self.layer4 = self._make_layers(64, 128, 3, 1)\n",
    "        self.fc = nn.Linear(128, 11)\n",
    "\n",
    "    def _make_layers(self, in_ch, out_ch, kernel_size, n):\n",
    "        dilation_rates = [2 ** i for i in range(n)]\n",
    "        layers = [nn.Conv1d(in_ch, out_ch, 1)]\n",
    "        for dilation in dilation_rates:\n",
    "            layers.append(self.basic_block(out_ch, out_ch, kernel_size, dilation))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
    "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
    "        self.counter, self.best_score = 0, None\n",
    "        self.is_maximize = is_maximize\n",
    "\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or \\\n",
    "                (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "            self.best_score, self.counter = score, 0\n",
    "            return 1\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return 2\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class IronDataset(Dataset):\n",
    "    def __init__(self, data, labels, training=True, transform=None, seq_len=5000, flip=0.5, noise_level=0, class_split=0.0):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.training = training\n",
    "        self.flip = flip\n",
    "        self.noise_level = noise_level\n",
    "        self.class_split = class_split\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        return [data.astype(np.float32), labels.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    train = pd.read_csv('/SSD4T/lyh/liverpool/train_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n",
    "    test  = pd.read_csv('/SSD4T/lyh/liverpool/test_clean_kalman.csv', dtype={'time': np.float32, 'signal': np.float32})\n",
    "    sub  = pd.read_csv('/SSD4T/lyh/liverpool/sample_submission.csv', dtype={'time': np.float32})\n",
    "    return train, test, sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data Started...\n",
      "Reading and Normalizing Data Completed\n",
      "Creating Features\n",
      "Feature Engineering Started...\n",
      "     time    signal  open_channels  group  signal_shift_pos_1  \\\n",
      "0  0.0001 -1.145101              0      0            0.000000   \n",
      "1  0.0002 -1.180292              0      0           -1.145101   \n",
      "2  0.0003 -1.009653              0      0           -1.180292   \n",
      "3  0.0004 -1.293866              0      0           -1.009653   \n",
      "4  0.0005 -1.299834              0      0           -1.293866   \n",
      "\n",
      "   signal_shift_neg_1  signal_shift_pos_2  signal_shift_neg_2  \\\n",
      "0           -1.180292            0.000000           -1.009653   \n",
      "1           -1.009653            0.000000           -1.293866   \n",
      "2           -1.293866           -1.145101           -1.299834   \n",
      "3           -1.299834           -1.180292           -1.100508   \n",
      "4           -1.100508           -1.009653           -1.119496   \n",
      "\n",
      "   signal_shift_pos_3  signal_shift_neg_3  signal_2  \n",
      "0            0.000000           -1.293866  1.311257  \n",
      "1            0.000000           -1.299834  1.393090  \n",
      "2            0.000000           -1.100508  1.019400  \n",
      "3           -1.145101           -1.119496  1.674090  \n",
      "4           -1.180292           -1.078828  1.689568  \n",
      "Feature Engineering Completed...\n",
      "     time    signal  open_channels  group  signal_shift_pos_1  \\\n",
      "0  0.0001 -1.145101              0      0            0.000000   \n",
      "1  0.0002 -1.180292              0      0           -1.145101   \n",
      "2  0.0003 -1.009653              0      0           -1.180292   \n",
      "3  0.0004 -1.293866              0      0           -1.009653   \n",
      "4  0.0005 -1.299834              0      0           -1.293866   \n",
      "\n",
      "   signal_shift_neg_1  signal_shift_pos_2  signal_shift_neg_2  \\\n",
      "0           -1.180292            0.000000           -1.009653   \n",
      "1           -1.009653            0.000000           -1.293866   \n",
      "2           -1.293866           -1.145101           -1.299834   \n",
      "3           -1.299834           -1.180292           -1.100508   \n",
      "4           -1.100508           -1.009653           -1.119496   \n",
      "\n",
      "   signal_shift_pos_3  signal_shift_neg_3  signal_2  \n",
      "0            0.000000           -1.293866  1.311257  \n",
      "1            0.000000           -1.299834  1.393090  \n",
      "2            0.000000           -1.100508  1.019400  \n",
      "3           -1.145101           -1.119496  1.674090  \n",
      "4           -1.180292           -1.078828  1.689568   (5000000, 11)\n",
      "(1250, 4000, 8) (500, 4000, 8) (1250, 4000, 1)\n"
     ]
    }
   ],
   "source": [
    "train, test, train_tr, new_splits = split()\n",
    "\n",
    "#train_tr = (train_tr > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n"
     ]
    }
   ],
   "source": [
    "test_y = np.zeros([int(2000000/GROUP_BATCH_SIZE), GROUP_BATCH_SIZE, 1])\n",
    "test_dataset = IronDataset(test, test_y, flip=False)\n",
    "test_dataloader = DataLoader(test_dataset, NNBATCHSIZE, shuffle=False, num_workers=0)\n",
    "test_preds_all = np.zeros((2000000, 2))\n",
    "\n",
    "\n",
    "\n",
    "oof_score = []\n",
    "for index, (train_index, val_index, _) in enumerate(new_splits[0:], start=0):\n",
    "    print(\"Fold : {}\".format(index))\n",
    "    train_dataset = IronDataset(train[train_index], train_tr[train_index], seq_len=GROUP_BATCH_SIZE, flip=flip, noise_level=noise)\n",
    "    train_dataloader = DataLoader(train_dataset, NNBATCHSIZE, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    valid_dataset = IronDataset(train[val_index], train_tr[val_index], seq_len=GROUP_BATCH_SIZE, flip=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, NNBATCHSIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    it = 0\n",
    "    model = Classifier()\n",
    "    model = model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=40, is_maximize=True,\n",
    "                                   checkpoint_path=os.path.join(outdir, \"gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index,\n",
    "                                                                                                             it)))\n",
    "\n",
    "    weight = None#cal_weights()\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "    schedular = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.7)\n",
    "    avg_train_losses, avg_valid_losses = [], []\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************\n",
      "Folder : 0 Epoch : 0\n",
      "Curr learning_rate: 0.001000000\n"
     ]
    }
   ],
   "source": [
    "    for epoch in range(EPOCHS):\n",
    "        print('**********************************')\n",
    "        print(\"Folder : {} Epoch : {}\".format(index, epoch))\n",
    "        print(\"Curr learning_rate: {:0.9f}\".format(optimizer.param_groups[0]['lr']))\n",
    "        train_losses, valid_losses = [], []\n",
    "        tr_loss_cls_item, val_loss_cls_item = [], []\n",
    "\n",
    "        model.train()  # prep model for training\n",
    "        train_preds, train_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()#.to(device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for x, y in (train_dataloader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x)\n",
    "\n",
    "            predictions_ = predictions.view(-1, predictions.shape[-1])\n",
    "            y_ = y.view(-1)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64000, 11])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtoolbox.nn import LabelSmoothingLoss\n",
    "\n",
    "classes = 11\n",
    "criterion = LabelSmoothingLoss(classes, smoothing=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(predictions_, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3356, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "**********************************\n",
      "Folder : 0 Epoch : 0\n",
      "Curr learning_rate: 0.001000000\n",
      "EVALUATION\n",
      "train_loss: 0.089444, valid_loss: 0.016211\n",
      "train_f1: 0.937298, valid_f1: 0.994433\n",
      "save folder 0 global val max f1 model score 0.994433\n",
      "**********************************\n",
      "Folder : 0 Epoch : 1\n",
      "Curr learning_rate: 0.001000000\n",
      "EVALUATION\n",
      "train_loss: 0.009771, valid_loss: 0.008149\n",
      "train_f1: 0.996391, valid_f1: 0.996494\n",
      "save folder 0 global val max f1 model score 0.996494\n",
      "**********************************\n",
      "Folder : 0 Epoch : 2\n",
      "Curr learning_rate: 0.001000000\n",
      "EVALUATION\n",
      "train_loss: 0.007098, valid_loss: 0.006218\n",
      "train_f1: 0.996916, valid_f1: 0.997112\n",
      "save folder 0 global val max f1 model score 0.997112\n",
      "**********************************\n",
      "Folder : 0 Epoch : 3\n",
      "Curr learning_rate: 0.001000000\n",
      "EVALUATION\n",
      "train_loss: 0.006390, valid_loss: 0.004987\n",
      "train_f1: 0.997200, valid_f1: 0.997604\n",
      "save folder 0 global val max f1 model score 0.997604\n",
      "**********************************\n",
      "Folder : 0 Epoch : 4\n",
      "Curr learning_rate: 0.001000000\n",
      "EVALUATION\n",
      "train_loss: 0.004485, valid_loss: 0.003862\n",
      "train_f1: 0.998018, valid_f1: 0.998126\n",
      "save folder 0 global val max f1 model score 0.998126\n",
      "**********************************\n",
      "Folder : 0 Epoch : 5\n",
      "Curr learning_rate: 0.001000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-b8369e431f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;31m#schedular.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# record training lossa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "        for x, y in (train_dataloader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(x)\n",
    "\n",
    "            predictions_ = predictions.view(-1, predictions.shape[-1])\n",
    "            y_ = y.view(-1)\n",
    "\n",
    "            loss = criterion(predictions_, y_)\n",
    "\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            #schedular.step()\n",
    "            # record training lossa\n",
    "            train_losses.append(loss.item())\n",
    "            train_true = torch.cat([train_true, y_], 0)\n",
    "            train_preds = torch.cat([train_preds, predictions_], 0)\n",
    "\n",
    "        model.eval()  # prep model for evaluation\n",
    "        val_preds, val_true = torch.Tensor([]).cuda(), torch.LongTensor([]).cuda()\n",
    "        print('EVALUATION')\n",
    "        with torch.no_grad():\n",
    "            for x, y in (valid_dataloader):\n",
    "                x = x.cuda()#.to(device)\n",
    "                y = y.cuda()#..to(device)\n",
    "\n",
    "                predictions = model(x)\n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1])\n",
    "                y_ = y.view(-1)\n",
    "\n",
    "                loss = criterion(predictions_, y_)\n",
    "\n",
    "                valid_losses.append(loss.item())\n",
    "\n",
    "\n",
    "                val_true = torch.cat([val_true, y_], 0)\n",
    "                val_preds = torch.cat([val_preds, predictions_], 0)\n",
    "\n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        print(\"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "\n",
    "        train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1),\n",
    "                               labels=list(range(2)), average='macro')\n",
    "\n",
    "        val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1),\n",
    "                             labels=list(range(2)), average='macro')\n",
    "\n",
    "        schedular.step(val_score)\n",
    "        print(\"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n",
    "        res = early_stopping(val_score, model)\n",
    "        #print('fres:', res)\n",
    "        if  res == 2:\n",
    "            print(\"Early Stopping\")\n",
    "            print('folder %d global best val max f1 model score %f' % (index, early_stopping.best_score))\n",
    "            break\n",
    "        elif res == 1:\n",
    "            print('save folder %d global val max f1 model score %f' % (index, val_score))\n",
    "    print('Folder {} finally best global max f1 score is {}'.format(index, early_stopping.best_score))\n",
    "    oof_score.append(round(early_stopping.best_score, 6))\n",
    "    \n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in (test_dataloader):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            predictions = model(x)\n",
    "            predictions_ = predictions.view(-1, predictions.shape[-1]) # shape [128, 4000, 11]\n",
    "            #print(predictions.shape, F.softmax(predictions_, dim=1).cpu().numpy().shape)\n",
    "            pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy()) # shape (512000, 11)\n",
    "            #a = input()\n",
    "        test_preds = np.vstack(pred_list) # shape [2000000, 11]\n",
    "        test_preds_all += test_preds\n",
    "        \n",
    "print('all folder score is:%s'%str(oof_score))\n",
    "print('OOF mean score is: %f'% (sum(oof_score)/len(oof_score)))\n",
    "print('Generate submission.............')\n",
    "submission_csv_path = '/kaggle/input/liverpool-ion-switching/sample_submission.csv'\n",
    "ss = pd.read_csv(submission_csv_path, dtype={'time': str})\n",
    "test_preds_all = test_preds_all / np.sum(test_preds_all, axis=1)[:, None]\n",
    "test_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n",
    "                                'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "test_pred_frame.to_csv(\"./gru_preds.csv\", index=False)\n",
    "print('over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
